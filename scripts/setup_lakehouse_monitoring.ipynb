{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6276bf22-717c-41ca-82a6-1e991538f870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakehouse Monitoring Setup\n",
    "\n",
    "This notebook automates the setup of Databricks Lakehouse Monitoring for data quality profiling.\n",
    "\n",
    "**Features:**\n",
    "- Create monitors for Delta tables\n",
    "- Configure profile and time series metrics\n",
    "- Set up drift detection\n",
    "- Schedule monitoring jobs\n",
    "- Access metrics and dashboards\n",
    "\n",
    "**Requirements:**\n",
    "- Unity Catalog enabled\n",
    "- Lakehouse Monitoring available\n",
    "- Delta tables to monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d18ff56a-329c-46a7-8382-53c2238d152c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5af30e9-85f1-4784-ba91-eededbb4eac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import databricks.lakehouse_monitoring as lm\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configuration parameters\n",
    "dbutils.widgets.text(\"catalog\", \"nonprod_natapcd\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema\", \"observability\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"table_to_monitor\", \"\", \"Table to Monitor (catalog.schema.table)\")\n",
    "dbutils.widgets.dropdown(\"monitor_type\", \"TimeSeries\", [\"TimeSeries\", \"Snapshot\", \"InferenceLog\"], \"Monitor Type\")\n",
    "dbutils.widgets.text(\"timestamp_col\", \"created_at\", \"Timestamp Column\")\n",
    "dbutils.widgets.text(\"granularity\", \"1 day\", \"Granularity\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_to_monitor = dbutils.widgets.get(\"table_to_monitor\")\n",
    "monitor_type = dbutils.widgets.get(\"monitor_type\")\n",
    "timestamp_col = dbutils.widgets.get(\"timestamp_col\")\n",
    "granularity = dbutils.widgets.get(\"granularity\")\n",
    "\n",
    "print(f\"Lakehouse Monitoring Configuration:\")\n",
    "print(f\"  Catalog: {catalog}\")\n",
    "print(f\"  Schema: {schema}\")\n",
    "print(f\"  Table to Monitor: {table_to_monitor}\")\n",
    "print(f\"  Monitor Type: {monitor_type}\")\n",
    "print(f\"  Timestamp Column: {timestamp_col}\")\n",
    "print(f\"  Granularity: {granularity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ba848b-0192-4e38-b1c0-230b9c51965c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7baea842-9c90-4abc-8299-327842862809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_timeseries_monitor(\n",
    "    table_name: str,\n",
    "    timestamp_col: str,\n",
    "    granularities: list = [\"1 day\"],\n",
    "    output_schema: str = None,\n",
    "    baseline_table: str = None,\n",
    "    slicing_exprs: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a time series monitor for a Delta table\n",
    "\n",
    "    Args:\n",
    "        table_name: Full table name (catalog.schema.table)\n",
    "        timestamp_col: Column to use for time series analysis\n",
    "        granularities: List of time granularities (e.g., [\"1 hour\", \"1 day\"])\n",
    "        output_schema: Schema to store monitoring metrics (defaults to same as table)\n",
    "        baseline_table: Optional baseline table for drift detection\n",
    "        slicing_exprs: Optional list of SQL expressions for data slicing\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Creating time series monitor for {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        monitor_info = lm.create_monitor(\n",
    "            table_name=table_name,\n",
    "            profile_type=lm.TimeSeries(\n",
    "                timestamp_col=timestamp_col,\n",
    "                granularities=granularities\n",
    "            ),\n",
    "            output_schema_name=output_schema,\n",
    "            baseline_table_name=baseline_table,\n",
    "            slicing_exprs=slicing_exprs\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Monitor created successfully!\")\n",
    "        print(f\"   Profile Metrics Table: {monitor_info.profile_metrics_table_name}\")\n",
    "        print(f\"   Drift Metrics Table: {monitor_info.drift_metrics_table_name}\")\n",
    "        print(f\"   Dashboard: {monitor_info.dashboard_id}\")\n",
    "\n",
    "        return monitor_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_snapshot_monitor(\n",
    "    table_name: str,\n",
    "    output_schema: str = None,\n",
    "    baseline_table: str = None,\n",
    "    slicing_exprs: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a snapshot monitor for a Delta table\n",
    "\n",
    "    Args:\n",
    "        table_name: Full table name (catalog.schema.table)\n",
    "        output_schema: Schema to store monitoring metrics\n",
    "        baseline_table: Optional baseline table for drift detection\n",
    "        slicing_exprs: Optional list of SQL expressions for data slicing\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Creating snapshot monitor for {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        monitor_info = lm.create_monitor(\n",
    "            table_name=table_name,\n",
    "            profile_type=lm.Snapshot(),\n",
    "            output_schema_name=output_schema,\n",
    "            baseline_table_name=baseline_table,\n",
    "            slicing_exprs=slicing_exprs\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Monitor created successfully!\")\n",
    "        print(f\"   Profile Metrics Table: {monitor_info.profile_metrics_table_name}\")\n",
    "        print(f\"   Drift Metrics Table: {monitor_info.drift_metrics_table_name}\")\n",
    "        print(f\"   Dashboard: {monitor_info.dashboard_id}\")\n",
    "\n",
    "        return monitor_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_inference_monitor(\n",
    "    table_name: str,\n",
    "    timestamp_col: str,\n",
    "    model_id_col: str,\n",
    "    prediction_col: str,\n",
    "    label_col: str = None,\n",
    "    granularities: list = [\"1 day\"],\n",
    "    output_schema: str = None,\n",
    "    problem_type: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an inference log monitor for ML model predictions\n",
    "\n",
    "    Args:\n",
    "        table_name: Full table name (catalog.schema.table)\n",
    "        timestamp_col: Column with prediction timestamp\n",
    "        model_id_col: Column with model identifier\n",
    "        prediction_col: Column with model predictions\n",
    "        label_col: Optional column with ground truth labels\n",
    "        granularities: List of time granularities\n",
    "        output_schema: Schema to store monitoring metrics\n",
    "        problem_type: ML problem type (classification, regression)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Creating inference log monitor for {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        monitor_info = lm.create_monitor(\n",
    "            table_name=table_name,\n",
    "            profile_type=lm.InferenceLog(\n",
    "                timestamp_col=timestamp_col,\n",
    "                model_id_col=model_id_col,\n",
    "                prediction_col=prediction_col,\n",
    "                label_col=label_col,\n",
    "                granularities=granularities,\n",
    "                problem_type=problem_type\n",
    "            ),\n",
    "            output_schema_name=output_schema\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Monitor created successfully!\")\n",
    "        print(f\"   Profile Metrics Table: {monitor_info.profile_metrics_table_name}\")\n",
    "        print(f\"   Drift Metrics Table: {monitor_info.drift_metrics_table_name}\")\n",
    "        print(f\"   Dashboard: {monitor_info.dashboard_id}\")\n",
    "\n",
    "        return monitor_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_monitor_info(table_name: str):\n",
    "    \"\"\"Get information about an existing monitor\"\"\"\n",
    "    try:\n",
    "        monitor_info = lm.get_monitor(table_name=table_name)\n",
    "        return monitor_info\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Monitor not found or error: {e}\")\n",
    "        return None\n",
    "\n",
    "def refresh_monitor(table_name: str):\n",
    "    \"\"\"Manually refresh monitor metrics\"\"\"\n",
    "    try:\n",
    "        refresh_info = lm.run_refresh(table_name=table_name)\n",
    "        print(f\"✅ Monitor refresh started for {table_name}\")\n",
    "        print(f\"   Refresh ID: {refresh_info.refresh_id}\")\n",
    "        return refresh_info\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error refreshing monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def delete_monitor(table_name: str):\n",
    "    \"\"\"Delete a monitor\"\"\"\n",
    "    try:\n",
    "        lm.delete_monitor(table_name=table_name)\n",
    "        print(f\"✅ Monitor deleted for {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deleting monitor: {e}\")\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a1f439-ebb6-4def-96ee-c46386915dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Monitor for Specified Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e961b4-ad06-47d4-ba0d-bcfb678a0e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if table_to_monitor:\n",
    "    # Check if monitor already exists\n",
    "    existing_monitor = get_monitor_info(table_to_monitor)\n",
    "\n",
    "    if existing_monitor:\n",
    "        print(f\"ℹ️  Monitor already exists for {table_to_monitor}\")\n",
    "        print(f\"   Status: {existing_monitor.status}\")\n",
    "        print(f\"   Profile Metrics: {existing_monitor.profile_metrics_table_name}\")\n",
    "        print(f\"   Dashboard: {existing_monitor.dashboard_id}\")\n",
    "\n",
    "        # Optionally refresh\n",
    "        refresh = dbutils.widgets.get(\"refresh_if_exists\")\n",
    "        if refresh and refresh.lower() == \"true\":\n",
    "            refresh_monitor(table_to_monitor)\n",
    "    else:\n",
    "        # Create new monitor based on type\n",
    "        output_schema = f\"{catalog}.{schema}\"\n",
    "\n",
    "        if monitor_type == \"TimeSeries\":\n",
    "            monitor_info = create_timeseries_monitor(\n",
    "                table_name=table_to_monitor,\n",
    "                timestamp_col=timestamp_col,\n",
    "                granularities=[granularity],\n",
    "                output_schema=output_schema\n",
    "            )\n",
    "        elif monitor_type == \"Snapshot\":\n",
    "            monitor_info = create_snapshot_monitor(\n",
    "                table_name=table_to_monitor,\n",
    "                output_schema=output_schema\n",
    "            )\n",
    "        else:\n",
    "            print(f\"⚠️  Monitor type {monitor_type} requires additional configuration\")\n",
    "else:\n",
    "    print(\"ℹ️  No table specified. Use the 'table_to_monitor' widget to specify a table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4523b8-b973-4ebc-b3b0-f7e23014cac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bulk Monitor Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b39866e-8e47-4108-8974-b840c4d405d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def setup_monitors_for_schema(\n",
    "    catalog_name: str,\n",
    "    schema_name: str,\n",
    "    timestamp_col_map: dict = None,\n",
    "    exclude_tables: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Set up monitors for all tables in a schema\n",
    "\n",
    "    Args:\n",
    "        catalog_name: Catalog name\n",
    "        schema_name: Schema name\n",
    "        timestamp_col_map: Dict mapping table names to timestamp columns\n",
    "        exclude_tables: List of table names to exclude\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all tables in schema\n",
    "    tables = spark.sql(f\"\"\"\n",
    "        SHOW TABLES IN {catalog_name}.{schema_name}\n",
    "    \"\"\").collect()\n",
    "\n",
    "    exclude_tables = exclude_tables or []\n",
    "    results = []\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table.tableName\n",
    "\n",
    "        if table_name in exclude_tables:\n",
    "            print(f\"⏭️  Skipping {table_name} (excluded)\")\n",
    "            continue\n",
    "\n",
    "        full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "        # Check if already monitored\n",
    "        if get_monitor_info(full_table_name):\n",
    "            print(f\"ℹ️  {table_name} already has a monitor\")\n",
    "            results.append({\"table\": table_name, \"status\": \"exists\"})\n",
    "            continue\n",
    "\n",
    "        # Determine timestamp column\n",
    "        timestamp_col = timestamp_col_map.get(table_name) if timestamp_col_map else None\n",
    "\n",
    "        if timestamp_col:\n",
    "            # Create time series monitor\n",
    "            monitor_info = create_timeseries_monitor(\n",
    "                table_name=full_table_name,\n",
    "                timestamp_col=timestamp_col,\n",
    "                granularities=[\"1 day\"],\n",
    "                output_schema=f\"{catalog_name}.{schema}\"\n",
    "            )\n",
    "            results.append({\"table\": table_name, \"status\": \"created_timeseries\"})\n",
    "        else:\n",
    "            # Create snapshot monitor\n",
    "            monitor_info = create_snapshot_monitor(\n",
    "                table_name=full_table_name,\n",
    "                output_schema=f\"{catalog_name}.{schema}\"\n",
    "            )\n",
    "            results.append({\"table\": table_name, \"status\": \"created_snapshot\"})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example: Set up monitors for all tables in a schema\n",
    "# Uncomment and customize as needed\n",
    "\"\"\"\n",
    "timestamp_columns = {\n",
    "    \"customers\": \"created_at\",\n",
    "    \"orders\": \"order_date\",\n",
    "    \"transactions\": \"transaction_timestamp\"\n",
    "}\n",
    "\n",
    "results = setup_monitors_for_schema(\n",
    "    catalog_name=catalog,\n",
    "    schema_name=\"bronze\",\n",
    "    timestamp_col_map=timestamp_columns,\n",
    "    exclude_tables=[\"temp_table\", \"staging_table\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Setup complete for {len(results)} tables\")\n",
    "for result in results:\n",
    "    print(f\"   {result['table']}: {result['status']}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb7ccde-3e61-41ba-8593-99e9c20d2bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query Monitoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6dcc89f-9dbf-4cad-aa57-692dcef32a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_profile_metrics(table_name: str, limit: int = 100):\n",
    "    \"\"\"Get profile metrics for a monitored table\"\"\"\n",
    "\n",
    "    monitor_info = get_monitor_info(table_name)\n",
    "\n",
    "    if not monitor_info:\n",
    "        print(f\"⚠️  No monitor found for {table_name}\")\n",
    "        return None\n",
    "\n",
    "    profile_table = monitor_info.profile_metrics_table_name\n",
    "\n",
    "    metrics_df = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {profile_table}\n",
    "        ORDER BY window.start DESC\n",
    "        LIMIT {limit}\n",
    "    \"\"\")\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def get_drift_metrics(table_name: str, limit: int = 100):\n",
    "    \"\"\"Get drift metrics for a monitored table\"\"\"\n",
    "\n",
    "    monitor_info = get_monitor_info(table_name)\n",
    "\n",
    "    if not monitor_info:\n",
    "        print(f\"⚠️  No monitor found for {table_name}\")\n",
    "        return None\n",
    "\n",
    "    drift_table = monitor_info.drift_metrics_table_name\n",
    "\n",
    "    drift_df = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {drift_table}\n",
    "        ORDER BY window.start DESC\n",
    "        LIMIT {limit}\n",
    "    \"\"\")\n",
    "\n",
    "    return drift_df\n",
    "\n",
    "# Example: Query metrics for the monitored table\n",
    "if table_to_monitor:\n",
    "    print(f\"\\n📊 Profile Metrics for {table_to_monitor}:\")\n",
    "    profile_metrics = get_profile_metrics(table_to_monitor)\n",
    "\n",
    "    if profile_metrics:\n",
    "        display(profile_metrics)\n",
    "\n",
    "    print(f\"\\n📊 Drift Metrics for {table_to_monitor}:\")\n",
    "    drift_metrics = get_drift_metrics(table_to_monitor)\n",
    "\n",
    "    if drift_metrics:\n",
    "        display(drift_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a6578c-13f4-4c8e-b2eb-2e07f0095b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monitoring Analysis Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cdf9187-be68-4210-afdf-dc2cde0272fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Column Statistics Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb9ebb44-afd3-41ac-8e2b-a60d540cc29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if table_to_monitor:\n",
    "    monitor_info = get_monitor_info(table_to_monitor)\n",
    "\n",
    "    if monitor_info:\n",
    "        spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          window.start as time_window,\n",
    "          column_name,\n",
    "          null_count,\n",
    "          null_percentage,\n",
    "          num_distinct,\n",
    "          min,\n",
    "          max,\n",
    "          avg,\n",
    "          stddev\n",
    "        FROM {monitor_info.profile_metrics_table_name}\n",
    "        ORDER BY window.start DESC, column_name\n",
    "        \"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec283c3-f5a5-4a0c-af1a-2808e320017c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drift Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8846366-46bb-4518-9746-813eeffc78b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if table_to_monitor:\n",
    "    monitor_info = get_monitor_info(table_to_monitor)\n",
    "\n",
    "    if monitor_info:\n",
    "        spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          window.start as time_window,\n",
    "          column_name,\n",
    "          drift_type,\n",
    "          drift_score,\n",
    "          threshold,\n",
    "          CASE\n",
    "            WHEN drift_score > threshold THEN 'DRIFT_DETECTED'\n",
    "            ELSE 'NO_DRIFT'\n",
    "          END as drift_status\n",
    "        FROM {monitor_info.drift_metrics_table_name}\n",
    "        WHERE drift_score IS NOT NULL\n",
    "        ORDER BY drift_score DESC, window.start DESC\n",
    "        \"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f440d7b9-ba91-4b08-a4ee-fbfdf1489a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schedule Monitor Refreshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00526e74-3b86-4fe9-8c5c-6be8642c6f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Monitoring Job\n",
    "\n",
    "To schedule regular monitor refreshes, create a Databricks job that:\n",
    "1. Runs this notebook or calls `lm.run_refresh()` for each monitored table\n",
    "2. Schedules based on data freshness requirements (hourly, daily, etc.)\n",
    "3. Sends notifications on failures\n",
    "\n",
    "Example job configuration:\n",
    "```python\n",
    "{\n",
    "  \"name\": \"Lakehouse Monitoring Refresh\",\n",
    "  \"tasks\": [\n",
    "    {\n",
    "      \"task_key\": \"refresh_monitors\",\n",
    "      \"notebook_task\": {\n",
    "        \"notebook_path\": \"/path/to/this/notebook\",\n",
    "        \"base_parameters\": {\n",
    "          \"catalog\": \"main\",\n",
    "          \"schema\": \"observability\",\n",
    "          \"table_to_monitor\": \"main.bronze.customers\"\n",
    "        }\n",
    "      },\n",
    "      \"existing_cluster_id\": \"xxx-xxxxxx-xxxxxxx\"\n",
    "    }\n",
    "  ],\n",
    "  \"schedule\": {\n",
    "    \"quartz_cron_expression\": \"0 0 * * * ?\",\n",
    "    \"timezone_id\": \"America/Los_Angeles\"\n",
    "  },\n",
    "  \"email_notifications\": {\n",
    "    \"on_failure\": [\"data-team@company.com\"]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e2296ab-ef48-4bbf-8143-7a766229c614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## List All Monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dd7abab-b09d-4685-a62e-6ec9cf837d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_all_monitors(catalog_name: str = None, schema_name: str = None):\n",
    "    \"\"\"\n",
    "    List all monitors in the workspace or specific catalog/schema\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"SHOW MONITORS\"\n",
    "\n",
    "    if catalog_name and schema_name:\n",
    "        query = f\"SHOW MONITORS IN {catalog_name}.{schema_name}\"\n",
    "    elif catalog_name:\n",
    "        query = f\"SHOW MONITORS IN {catalog_name}\"\n",
    "\n",
    "    try:\n",
    "        monitors_df = spark.sql(query)\n",
    "        return monitors_df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error listing monitors: {e}\")\n",
    "        return None\n",
    "\n",
    "# List all monitors\n",
    "all_monitors = list_all_monitors(catalog)\n",
    "\n",
    "if all_monitors:\n",
    "    print(f\"📊 Active Monitors:\")\n",
    "    display(all_monitors)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "setup_lakehouse_monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
