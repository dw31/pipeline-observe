{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakehouse Monitoring Setup\n",
    "\n",
    "This notebook automates the setup of Databricks Lakehouse Monitoring for data quality profiling.\n",
    "\n",
    "**Features:**\n",
    "- Create monitors for Delta tables using the Databricks SDK\n",
    "- Configure TimeSeries, Snapshot, and InferenceLog monitors\n",
    "- Set up drift detection and baseline comparisons\n",
    "- Schedule monitoring jobs\n",
    "- Access metrics and dashboards\n",
    "\n",
    "**Requirements:**\n",
    "- Unity Catalog enabled\n",
    "- Lakehouse Monitoring available\n",
    "- Delta tables to monitor\n",
    "- Databricks SDK for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import (\n",
    "    MonitorTimeSeries,\n",
    "    MonitorSnapshot,\n",
    "    MonitorInferenceLog,\n",
    "    MonitorInferenceLogProblemType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "# Initialize Databricks Workspace Client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get current user email for assets directory\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "\n",
    "# Configuration parameters\n",
    "dbutils.widgets.text(\"catalog\", \"nonprod_natapcd\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema\", \"observability\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"table_to_monitor\", \"\", \"Table to Monitor (catalog.schema.table)\")\n",
    "dbutils.widgets.dropdown(\"monitor_type\", \"TimeSeries\", [\"TimeSeries\", \"Snapshot\", \"InferenceLog\"], \"Monitor Type\")\n",
    "dbutils.widgets.text(\"timestamp_col\", \"created_at\", \"Timestamp Column\")\n",
    "dbutils.widgets.text(\"granularity\", \"1 day\", \"Granularity\")\n",
    "dbutils.widgets.dropdown(\"refresh_if_exists\", \"false\", [\"true\", \"false\"], \"Refresh if Monitor Exists\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_to_monitor = dbutils.widgets.get(\"table_to_monitor\")\n",
    "monitor_type = dbutils.widgets.get(\"monitor_type\")\n",
    "timestamp_col = dbutils.widgets.get(\"timestamp_col\")\n",
    "granularity = dbutils.widgets.get(\"granularity\")\n",
    "\n",
    "print(f\"Lakehouse Monitoring Configuration:\")\n",
    "print(f\"  Catalog: {catalog}\")\n",
    "print(f\"  Schema: {schema}\")\n",
    "print(f\"  Table to Monitor: {table_to_monitor}\")\n",
    "print(f\"  Monitor Type: {monitor_type}\")\n",
    "print(f\"  Timestamp Column: {timestamp_col}\")\n",
    "print(f\"  Granularity: {granularity}\")\n",
    "print(f\"  User: {user_email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assets_dir(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate assets directory path for monitor\n",
    "    \n",
    "    Args:\n",
    "        table_name: Full table name (catalog.schema.table)\n",
    "    \n",
    "    Returns:\n",
    "        Path for monitor assets in Workspace\n",
    "    \"\"\"\n",
    "    # Sanitize table name for path\n",
    "    safe_table_name = re.sub(r'[^a-zA-Z0-9_.]', '_', table_name)\n",
    "    return f\"/Workspace/Users/{user_email}/databricks_lakehouse_monitoring/{safe_table_name}\"\n",
    "\n",
    "def create_timeseries_monitor(\n",
    "    table_name: str,\n",
    "    timestamp_col: str,\n",
    "    granularities: list = [\"1 day\"],\n",
    "    output_schema: str = None,\n",
    "    baseline_table: str = None,\n",
    "    slicing_exprs: list = None,\n",
    "    custom_metrics: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a time series monitor for a Delta table\n",
    "\n",
    "    Args:\n",
    "        table_name: Full table name (catalog.schema.table)\n",
    "        timestamp_col: Column to use for time series analysis\n",
    "        granularities: List of time granularities (e.g., [\"1 hour\", \"1 day\"])\n",
    "        output_schema: Schema to store monitoring metrics (defaults to table's schema)\n",
    "        baseline_table: Optional baseline table for drift detection\n",
    "        slicing_exprs: Optional list of SQL expressions for data slicing\n",
    "        custom_metrics: Optional list of custom metric definitions\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Creating time series monitor for {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        assets_dir = get_assets_dir(table_name)\n",
    "        \n",
    "        monitor_info = w.quality_monitors.create(\n",
    "            table_name=table_name,\n",
    "            assets_dir=assets_dir,\n",
    "            output_schema_name=output_schema,\n",
    "            time_series=MonitorTimeSeries(\n",
    "                timestamp_col=timestamp_col,\n",
    "                granularities=granularities\n",
    "            ),\n",
    "            baseline_table_name=baseline_table,\n",
    "            slicing_exprs=slicing_exprs,\n",
    "            custom_metrics=custom_metrics\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Monitor created successfully!\")\n",
    "        print(f\"   Monitor ID: {monitor_info.monitor_version}\")\n",
    "        print(f\"   Profile Metrics Table: {monitor_info.profile_metrics_table_name}\")\n",
    "        print(f\"   Drift Metrics Table: {monitor_info.drift_metrics_table_name}\")\n",
    "        print(f\"   Dashboard ID: {monitor_info.dashboard_id}\")\n",
    "        print(f\"   Assets Directory: {assets_dir}\")\n",
    "\n",
    "        return monitor_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_snapshot_monitor(\n",
    "    table_name: str,\n",
    "    output_schema: str = None,\n",
    "    baseline_table: str = None,\n",
    "    slicing_exprs: list = None,\n",
    "    custom_metrics: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a snapshot monitor for a Delta table\n",
    "\n",
    "    Args:\n",
    "        table_name: Full table name (catalog.schema.table)\n",
    "        output_schema: Schema to store monitoring metrics\n",
    "        baseline_table: Optional baseline table for drift detection\n",
    "        slicing_exprs: Optional list of SQL expressions for data slicing\n",
    "        custom_metrics: Optional list of custom metric definitions\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Creating snapshot monitor for {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        assets_dir = get_assets_dir(table_name)\n",
    "        \n",
    "        monitor_info = w.quality_monitors.create(\n",
    "            table_name=table_name,\n",
    "            assets_dir=assets_dir,\n",
    "            output_schema_name=output_schema,\n",
    "            snapshot=MonitorSnapshot(),\n",
    "            baseline_table_name=baseline_table,\n",
    "            slicing_exprs=slicing_exprs,\n",
    "            custom_metrics=custom_metrics\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Monitor created successfully!\")\n",
    "        print(f\"   Monitor ID: {monitor_info.monitor_version}\")\n",
    "        print(f\"   Profile Metrics Table: {monitor_info.profile_metrics_table_name}\")\n",
    "        print(f\"   Drift Metrics Table: {monitor_info.drift_metrics_table_name}\")\n",
    "        print(f\"   Dashboard ID: {monitor_info.dashboard_id}\")\n",
    "        print(f\"   Assets Directory: {assets_dir}\")\n",
    "\n",
    "        return monitor_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_inference_monitor(\n",
    "    table_name: str,\n",
    "    timestamp_col: str,\n",
    "    model_id_col: str,\n",
    "    prediction_col: str,\n",
    "    problem_type: str,\n",
    "    label_col: str = None,\n",
    "    granularities: list = [\"1 day\"],\n",
    "    output_schema: str = None,\n",
    "    custom_metrics: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an inference log monitor for ML model predictions\n",
    "\n",
    "    Args:\n",
    "        table_name: Full table name (catalog.schema.table)\n",
    "        timestamp_col: Column with prediction timestamp\n",
    "        model_id_col: Column with model identifier\n",
    "        prediction_col: Column with model predictions\n",
    "        problem_type: ML problem type ('classification' or 'regression')\n",
    "        label_col: Optional column with ground truth labels\n",
    "        granularities: List of time granularities\n",
    "        output_schema: Schema to store monitoring metrics\n",
    "        custom_metrics: Optional list of custom metric definitions\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Creating inference log monitor for {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        # Convert problem_type string to enum\n",
    "        if problem_type.lower() == 'classification':\n",
    "            problem_type_enum = MonitorInferenceLogProblemType.PROBLEM_TYPE_CLASSIFICATION\n",
    "        elif problem_type.lower() == 'regression':\n",
    "            problem_type_enum = MonitorInferenceLogProblemType.PROBLEM_TYPE_REGRESSION\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid problem_type: {problem_type}. Must be 'classification' or 'regression'\")\n",
    "        \n",
    "        assets_dir = get_assets_dir(table_name)\n",
    "        \n",
    "        monitor_info = w.quality_monitors.create(\n",
    "            table_name=table_name,\n",
    "            assets_dir=assets_dir,\n",
    "            output_schema_name=output_schema,\n",
    "            inference_log=MonitorInferenceLog(\n",
    "                problem_type=problem_type_enum,\n",
    "                prediction_col=prediction_col,\n",
    "                timestamp_col=timestamp_col,\n",
    "                granularities=granularities,\n",
    "                model_id_col=model_id_col,\n",
    "                label_col=label_col\n",
    "            ),\n",
    "            custom_metrics=custom_metrics\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Monitor created successfully!\")\n",
    "        print(f\"   Monitor ID: {monitor_info.monitor_version}\")\n",
    "        print(f\"   Profile Metrics Table: {monitor_info.profile_metrics_table_name}\")\n",
    "        print(f\"   Drift Metrics Table: {monitor_info.drift_metrics_table_name}\")\n",
    "        print(f\"   Dashboard ID: {monitor_info.dashboard_id}\")\n",
    "        print(f\"   Assets Directory: {assets_dir}\")\n",
    "\n",
    "        return monitor_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_monitor_info(table_name: str):\n",
    "    \"\"\"Get information about an existing monitor\"\"\"\n",
    "    try:\n",
    "        monitor_info = w.quality_monitors.get(table_name=table_name)\n",
    "        return monitor_info\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Monitor not found or error: {e}\")\n",
    "        return None\n",
    "\n",
    "def refresh_monitor(table_name: str):\n",
    "    \"\"\"Manually refresh monitor metrics\"\"\"\n",
    "    try:\n",
    "        refresh_info = w.quality_monitors.run_refresh(table_name=table_name)\n",
    "        print(f\"✅ Monitor refresh started for {table_name}\")\n",
    "        print(f\"   Refresh ID: {refresh_info.refresh_id}\")\n",
    "        return refresh_info\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error refreshing monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def update_monitor(\n",
    "    table_name: str,\n",
    "    output_schema: str = None,\n",
    "    baseline_table: str = None,\n",
    "    slicing_exprs: list = None,\n",
    "    custom_metrics: list = None\n",
    "):\n",
    "    \"\"\"Update an existing monitor's configuration\"\"\"\n",
    "    try:\n",
    "        monitor_info = w.quality_monitors.update(\n",
    "            table_name=table_name,\n",
    "            output_schema_name=output_schema,\n",
    "            baseline_table_name=baseline_table,\n",
    "            slicing_exprs=slicing_exprs,\n",
    "            custom_metrics=custom_metrics\n",
    "        )\n",
    "        print(f\"✅ Monitor updated for {table_name}\")\n",
    "        return monitor_info\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error updating monitor: {e}\")\n",
    "        return None\n",
    "\n",
    "def delete_monitor(table_name: str):\n",
    "    \"\"\"Delete a monitor\"\"\"\n",
    "    try:\n",
    "        w.quality_monitors.delete(table_name=table_name)\n",
    "        print(f\"✅ Monitor deleted for {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deleting monitor: {e}\")\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Monitor for Specified Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if table_to_monitor:\n",
    "    # Check if monitor already exists\n",
    "    existing_monitor = get_monitor_info(table_to_monitor)\n",
    "\n",
    "    if existing_monitor:\n",
    "        print(f\"ℹ️  Monitor already exists for {table_to_monitor}\")\n",
    "        print(f\"   Status: {existing_monitor.status}\")\n",
    "        print(f\"   Monitor Version: {existing_monitor.monitor_version}\")\n",
    "        print(f\"   Profile Metrics: {existing_monitor.profile_metrics_table_name}\")\n",
    "        print(f\"   Dashboard ID: {existing_monitor.dashboard_id}\")\n",
    "\n",
    "        # Optionally refresh\n",
    "        refresh = dbutils.widgets.get(\"refresh_if_exists\")\n",
    "        if refresh and refresh.lower() == \"true\":\n",
    "            refresh_monitor(table_to_monitor)\n",
    "    else:\n",
    "        # Create new monitor based on type\n",
    "        output_schema = f\"{catalog}.{schema}\"\n",
    "\n",
    "        if monitor_type == \"TimeSeries\":\n",
    "            monitor_info = create_timeseries_monitor(\n",
    "                table_name=table_to_monitor,\n",
    "                timestamp_col=timestamp_col,\n",
    "                granularities=[granularity],\n",
    "                output_schema=output_schema\n",
    "            )\n",
    "        elif monitor_type == \"Snapshot\":\n",
    "            monitor_info = create_snapshot_monitor(\n",
    "                table_name=table_to_monitor,\n",
    "                output_schema=output_schema\n",
    "            )\n",
    "        elif monitor_type == \"InferenceLog\":\n",
    "            # For InferenceLog, additional widgets are needed\n",
    "            print(f\"⚠️  InferenceLog monitor requires additional configuration:\")\n",
    "            print(f\"     - model_id_col: Column with model identifier\")\n",
    "            print(f\"     - prediction_col: Column with predictions\")\n",
    "            print(f\"     - problem_type: 'classification' or 'regression'\")\n",
    "            print(f\"     - label_col (optional): Column with ground truth labels\")\n",
    "            print(f\"\\n     Please use the create_inference_monitor() function directly\")\n",
    "        else:\n",
    "            print(f\"⚠️  Unknown monitor type: {monitor_type}\")\n",
    "else:\n",
    "    print(\"ℹ️  No table specified. Use the 'table_to_monitor' widget to specify a table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Monitor Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_monitors_for_schema(\n",
    "    catalog_name: str,\n",
    "    schema_name: str,\n",
    "    timestamp_col_map: dict = None,\n",
    "    exclude_tables: list = None,\n",
    "    output_schema: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Set up monitors for all tables in a schema\n",
    "\n",
    "    Args:\n",
    "        catalog_name: Catalog name\n",
    "        schema_name: Schema name\n",
    "        timestamp_col_map: Dict mapping table names to timestamp columns\n",
    "        exclude_tables: List of table names to exclude\n",
    "        output_schema: Schema for monitor output (defaults to catalog.observability)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all tables in schema\n",
    "    tables = spark.sql(f\"\"\"\n",
    "        SHOW TABLES IN {catalog_name}.{schema_name}\n",
    "    \"\"\").collect()\n",
    "\n",
    "    exclude_tables = exclude_tables or []\n",
    "    results = []\n",
    "    \n",
    "    # Default output schema\n",
    "    if not output_schema:\n",
    "        output_schema = f\"{catalog_name}.observability\"\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table.tableName\n",
    "\n",
    "        if table_name in exclude_tables:\n",
    "            print(f\"⏭️  Skipping {table_name} (excluded)\")\n",
    "            continue\n",
    "\n",
    "        full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "        # Check if already monitored\n",
    "        if get_monitor_info(full_table_name):\n",
    "            print(f\"ℹ️  {table_name} already has a monitor\")\n",
    "            results.append({\"table\": table_name, \"status\": \"exists\"})\n",
    "            continue\n",
    "\n",
    "        # Determine timestamp column\n",
    "        timestamp_col = timestamp_col_map.get(table_name) if timestamp_col_map else None\n",
    "\n",
    "        if timestamp_col:\n",
    "            # Create time series monitor\n",
    "            monitor_info = create_timeseries_monitor(\n",
    "                table_name=full_table_name,\n",
    "                timestamp_col=timestamp_col,\n",
    "                granularities=[\"1 day\"],\n",
    "                output_schema=output_schema\n",
    "            )\n",
    "            if monitor_info:\n",
    "                results.append({\"table\": table_name, \"status\": \"created_timeseries\"})\n",
    "            else:\n",
    "                results.append({\"table\": table_name, \"status\": \"failed\"})\n",
    "        else:\n",
    "            # Create snapshot monitor\n",
    "            monitor_info = create_snapshot_monitor(\n",
    "                table_name=full_table_name,\n",
    "                output_schema=output_schema\n",
    "            )\n",
    "            if monitor_info:\n",
    "                results.append({\"table\": table_name, \"status\": \"created_snapshot\"})\n",
    "            else:\n",
    "                results.append({\"table\": table_name, \"status\": \"failed\"})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example: Set up monitors for all tables in a schema\n",
    "# Uncomment and customize as needed\n",
    "\"\"\"\n",
    "timestamp_columns = {\n",
    "    \"customers\": \"created_at\",\n",
    "    \"orders\": \"order_date\",\n",
    "    \"transactions\": \"transaction_timestamp\"\n",
    "}\n",
    "\n",
    "results = setup_monitors_for_schema(\n",
    "    catalog_name=catalog,\n",
    "    schema_name=\"bronze\",\n",
    "    timestamp_col_map=timestamp_columns,\n",
    "    exclude_tables=[\"temp_table\", \"staging_table\"],\n",
    "    output_schema=f\"{catalog}.observability\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Setup complete for {len(results)} tables\")\n",
    "for result in results:\n",
    "    print(f\"   {result['table']}: {result['status']}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Monitoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile_metrics(table_name: str, limit: int = 100):\n",
    "    \"\"\"Get profile metrics for a monitored table\"\"\"\n",
    "\n",
    "    monitor_info = get_monitor_info(table_name)\n",
    "\n",
    "    if not monitor_info:\n",
    "        print(f\"⚠️  No monitor found for {table_name}\")\n",
    "        return None\n",
    "\n",
    "    profile_table = monitor_info.profile_metrics_table_name\n",
    "\n",
    "    metrics_df = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {profile_table}\n",
    "        ORDER BY window.start DESC\n",
    "        LIMIT {limit}\n",
    "    \"\"\")\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def get_drift_metrics(table_name: str, limit: int = 100):\n",
    "    \"\"\"Get drift metrics for a monitored table\"\"\"\n",
    "\n",
    "    monitor_info = get_monitor_info(table_name)\n",
    "\n",
    "    if not monitor_info:\n",
    "        print(f\"⚠️  No monitor found for {table_name}\")\n",
    "        return None\n",
    "\n",
    "    drift_table = monitor_info.drift_metrics_table_name\n",
    "\n",
    "    drift_df = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {drift_table}\n",
    "        ORDER BY window.start DESC\n",
    "        LIMIT {limit}\n",
    "    \"\"\")\n",
    "\n",
    "    return drift_df\n",
    "\n",
    "# Example: Query metrics for the monitored table\n",
    "if table_to_monitor:\n",
    "    existing_monitor = get_monitor_info(table_to_monitor)\n",
    "    \n",
    "    if existing_monitor:\n",
    "        print(f\"\\n📊 Profile Metrics for {table_to_monitor}:\")\n",
    "        profile_metrics = get_profile_metrics(table_to_monitor)\n",
    "\n",
    "        if profile_metrics:\n",
    "            display(profile_metrics)\n",
    "\n",
    "        print(f\"\\n📊 Drift Metrics for {table_to_monitor}:\")\n",
    "        drift_metrics = get_drift_metrics(table_to_monitor)\n",
    "\n",
    "        if drift_metrics:\n",
    "            display(drift_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Analysis Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Statistics Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if table_to_monitor:\n",
    "    monitor_info = get_monitor_info(table_to_monitor)\n",
    "\n",
    "    if monitor_info:\n",
    "        spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          window.start as time_window,\n",
    "          column_name,\n",
    "          null_count,\n",
    "          null_percentage,\n",
    "          num_distinct,\n",
    "          min,\n",
    "          max,\n",
    "          avg,\n",
    "          stddev\n",
    "        FROM {monitor_info.profile_metrics_table_name}\n",
    "        ORDER BY window.start DESC, column_name\n",
    "        \"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if table_to_monitor:\n",
    "    monitor_info = get_monitor_info(table_to_monitor)\n",
    "\n",
    "    if monitor_info and monitor_info.drift_metrics_table_name:\n",
    "        spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          window.start as time_window,\n",
    "          column_name,\n",
    "          drift_type,\n",
    "          drift_score,\n",
    "          threshold,\n",
    "          CASE\n",
    "            WHEN drift_score > threshold THEN 'DRIFT_DETECTED'\n",
    "            ELSE 'NO_DRIFT'\n",
    "          END as drift_status\n",
    "        FROM {monitor_info.drift_metrics_table_name}\n",
    "        WHERE drift_score IS NOT NULL\n",
    "        ORDER BY drift_score DESC, window.start DESC\n",
    "        \"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule Monitor Refreshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Job\n",
    "\n",
    "To schedule regular monitor refreshes, create a Databricks job that:\n",
    "1. Runs this notebook or calls `w.quality_monitors.run_refresh()` for each monitored table\n",
    "2. Schedules based on data freshness requirements (hourly, daily, etc.)\n",
    "3. Sends notifications on failures\n",
    "\n",
    "Example using Databricks SDK:\n",
    "\n",
    "```python\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import (\n",
    "    JobSettings,\n",
    "    Task,\n",
    "    NotebookTask,\n",
    "    CronSchedule,\n",
    "    JobEmailNotifications\n",
    ")\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "job = w.jobs.create(\n",
    "    name=\"Lakehouse Monitoring Refresh\",\n",
    "    tasks=[\n",
    "        Task(\n",
    "            task_key=\"refresh_monitors\",\n",
    "            notebook_task=NotebookTask(\n",
    "                notebook_path=\"/path/to/this/notebook\",\n",
    "                base_parameters={\n",
    "                    \"catalog\": \"main\",\n",
    "                    \"schema\": \"observability\",\n",
    "                    \"table_to_monitor\": \"main.bronze.customers\",\n",
    "                    \"refresh_if_exists\": \"true\"\n",
    "                }\n",
    "            ),\n",
    "            existing_cluster_id=\"xxx-xxxxxx-xxxxxxx\"\n",
    "        )\n",
    "    ],\n",
    "    schedule=CronSchedule(\n",
    "        quartz_cron_expression=\"0 0 * * * ?\",\n",
    "        timezone_id=\"America/Los_Angeles\"\n",
    "    ),\n",
    "    email_notifications=JobEmailNotifications(\n",
    "        on_failure=[\"data-team@company.com\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Created job: {job.job_id}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_monitors(catalog_name: str = None, schema_name: str = None):\n",
    "    \"\"\"\n",
    "    List all monitors in the workspace or specific catalog/schema\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Using Databricks SDK to list monitors\n",
    "        monitors = w.quality_monitors.list_monitors()\n",
    "        \n",
    "        monitor_list = []\n",
    "        for monitor in monitors:\n",
    "            # Filter by catalog/schema if specified\n",
    "            if catalog_name and not monitor.table_name.startswith(f\"{catalog_name}.\"):\n",
    "                continue\n",
    "            if schema_name and not monitor.table_name.startswith(f\"{catalog_name}.{schema_name}.\"):\n",
    "                continue\n",
    "                \n",
    "            monitor_list.append({\n",
    "                \"table_name\": monitor.table_name,\n",
    "                \"status\": monitor.status,\n",
    "                \"monitor_version\": monitor.monitor_version,\n",
    "                \"dashboard_id\": monitor.dashboard_id\n",
    "            })\n",
    "        \n",
    "        return spark.createDataFrame(monitor_list) if monitor_list else None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error listing monitors: {e}\")\n",
    "        return None\n",
    "\n",
    "# List all monitors\n",
    "all_monitors = list_all_monitors(catalog)\n",
    "\n",
    "if all_monitors:\n",
    "    print(f\"📊 Active Monitors:\")\n",
    "    display(all_monitors)\n",
    "else:\n",
    "    print(f\"ℹ️  No monitors found in catalog '{catalog}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Create Inference Log Monitor\n",
    "\n",
    "Uncomment and customize for ML inference monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Example: Monitor ML model predictions\n",
    "inference_monitor = create_inference_monitor(\n",
    "    table_name=\"main.ml_models.customer_churn_predictions\",\n",
    "    timestamp_col=\"prediction_timestamp\",\n",
    "    model_id_col=\"model_version\",\n",
    "    prediction_col=\"churn_probability\",\n",
    "    problem_type=\"classification\",\n",
    "    label_col=\"actual_churn\",  # Optional: for accuracy tracking\n",
    "    granularities=[\"30 minutes\", \"1 day\"],\n",
    "    output_schema=\"main.observability\"\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
