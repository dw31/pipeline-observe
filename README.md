# Databricks Pipeline Observability and Data Quality Monitoring

A comprehensive observability and data quality monitoring solution for Databricks data pipelines, built using Delta Live Tables, Unity Catalog, and Lakehouse Monitoring.

## ğŸ¯ Overview

This project implements an end-to-end observability platform for Databricks data pipelines that provides:

- **Real-time Pipeline Monitoring**: Track DLT pipeline execution, performance, and health
- **Data Quality Management**: Centralized quality rules with automated validation and quarantine
- **Cost and Performance Analytics**: Monitor compute costs, resource utilization, and query performance
- **Data Lineage Tracking**: Automatic capture and visualization of table and column-level lineage
- **Intelligent Alerting**: Proactive notifications for quality issues, failures, and anomalies
- **Statistical Profiling**: Automated data profiling and drift detection using Lakehouse Monitoring

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Components](#components)
- [Configuration](#configuration)
- [Usage Examples](#usage-examples)
- [Monitoring Dashboards](#monitoring-dashboards)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)

## âœ¨ Features

### Data Collection and Integration
- âœ… Automated DLT event log collection and analysis
- âœ… Integration with Databricks system tables (billing, jobs, queries, clusters)
- âœ… Custom application-level logging with Python logging module
- âœ… Unity Catalog lineage capture (table and column level)

### Data Quality Monitoring
- âœ… Centralized quality rules management with Delta tables
- âœ… Dynamic expectation loading in DLT pipelines
- âœ… Multiple action types: warn, drop invalid records, fail pipeline
- âœ… Quarantine tables for invalid data review
- âœ… Quality score tracking and trending
- âœ… Template library for common validation patterns

### Observability and Reporting
- âœ… End-to-end data lineage visualization
- âœ… Impact analysis for upstream/downstream dependencies
- âœ… Performance metrics: backlog, throughput, latency
- âœ… Cost analysis: job costs, SKU breakdown, anomaly detection
- âœ… Cluster utilization monitoring
- âœ… Query performance analysis

### Statistical Profiling
- âœ… Lakehouse Monitoring setup automation
- âœ… Time series and snapshot monitoring
- âœ… Drift detection against baseline tables
- âœ… Column-level statistics tracking
- âœ… Automated dashboard generation

### Alerting and Notifications
- âœ… Multi-channel notifications (Email, Slack, Teams)
- âœ… Configurable alert thresholds
- âœ… DBSQL alerts for scheduled monitoring
- âœ… Job-level failure and SLA alerts
- âœ… Alert history tracking

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Sources                              â”‚
â”‚  DLT Pipelines â”‚ Jobs â”‚ Queries â”‚ Clusters â”‚ Tables         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Data Collection Layer                       â”‚
â”‚  â€¢ DLT Event Logs        â€¢ Unity Catalog Lineage            â”‚
â”‚  â€¢ System Tables         â€¢ Custom App Logs                  â”‚
â”‚  â€¢ Lakehouse Monitoring  â€¢ Query History                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Observability Data Store (Delta)                â”‚
â”‚  â€¢ Event Logs            â€¢ Quality Metrics                  â”‚
â”‚  â€¢ Job Metrics           â€¢ Cost Metrics                     â”‚
â”‚  â€¢ Query Metrics         â€¢ Lineage Metadata                 â”‚
â”‚  â€¢ Pipeline Health       â€¢ Alert History                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Analytics and Alerting Layer                       â”‚
â”‚  â€¢ DBSQL Dashboards      â€¢ Scheduled Alerts                 â”‚
â”‚  â€¢ Ad-hoc Analysis       â€¢ Webhook Notifications            â”‚
â”‚  â€¢ Lineage Visualization â€¢ Custom Reports                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

1. **DLT Pipelines with Quality Gates**: Bronze/Silver/Gold architecture with expectations
2. **Centralized Quality Rules**: Delta table storing reusable validation rules
3. **Monitoring Notebooks**: Collect and analyze observability data
4. **Lakehouse Monitoring**: Automated statistical profiling and drift detection
5. **Alert System**: Multi-channel notifications for SLA violations
6. **Lineage Tracking**: Unity Catalog-based lineage capture and visualization

## ğŸ“¦ Prerequisites

### Required
- Databricks workspace (AWS, Azure, or GCP)
- Unity Catalog enabled
- Databricks Runtime 13.3 LTS or higher
- System tables enabled (contact Databricks account admin)
- Delta Live Tables capability

### Recommended
- DBSQL warehouse for dashboards and alerts
- Photon enabled for better performance
- Enhanced autoscaling for DLT pipelines

### Permissions Required
- Create and manage tables in Unity Catalog
- Create and run DLT pipelines
- Access system tables (`system.billing`, `system.lakeflow`, `system.access`)
- Create DBSQL queries and alerts
- Configure job-level notifications

## ğŸš€ Installation

### Step 1: Clone or Import Project

Import this project into your Databricks workspace:

```bash
# Using Databricks CLI
databricks workspace import-dir . /Workspace/Users/<your-email>/pipeline-observe

# Or upload via Databricks UI: Workspace â†’ Import
```

### Step 2: Create Observability Catalog and Schema

Run the setup notebook to create required schemas:

```python
# In a Databricks notebook
%run ./scripts/setup_observability_schema
```

Or manually:

```sql
-- Create catalog and schema
CREATE CATALOG IF NOT EXISTS main;
CREATE SCHEMA IF NOT EXISTS main.observability;

-- Grant permissions
GRANT USE CATALOG ON CATALOG main TO `data-engineers`;
GRANT USE SCHEMA, CREATE TABLE ON SCHEMA main.observability TO `data-engineers`;
```

### Step 3: Initialize Quality Rules

Run the quality rules manager notebook:

```bash
# Navigate to: notebooks/data_quality/quality_rules_manager.py
# Run all cells to create rules table and sample rules
```

### Step 4: Deploy Sample DLT Pipeline (Optional)

1. Navigate to **Workflows** â†’ **Delta Live Tables**
2. Click **Create Pipeline**
3. Configure:
   - **Name**: Sample Observability Pipeline
   - **Notebook**: `notebooks/dlt_pipelines/sample_dlt_pipeline.py`
   - **Target**: `main.sample_pipeline`
   - **Storage**: `/mnt/dlt/sample_pipeline`
4. Click **Create** and **Start**

### Step 5: Set Up Monitoring Jobs

Create Databricks jobs to run monitoring notebooks on schedule:

```python
# Example job configuration for DLT monitoring
{
  "name": "DLT Event Log Collection",
  "tasks": [{
    "task_key": "collect_logs",
    "notebook_task": {
      "notebook_path": "/Workspace/Users/<email>/pipeline-observe/notebooks/monitoring/dlt_event_log_monitoring",
      "base_parameters": {
        "catalog": "main",
        "schema": "observability",
        "days_back": "7"
      }
    },
    "job_cluster_key": "monitoring_cluster"
  }],
  "schedule": {
    "quartz_cron_expression": "0 */6 * * * ?",  # Every 6 hours
    "timezone_id": "America/Los_Angeles"
  }
}
```

### Step 6: Configure Alerts

1. Navigate to **SQL** â†’ **Alerts**
2. Import alert queries from `sql/monitoring_dashboard_queries.sql`
3. Configure notification destinations (email, Slack, Teams)
4. Set alert schedules and thresholds

## ğŸ“ Project Structure

```
pipeline-observe/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ databricks_observability_and_data_quality_prd.md  # Product requirements
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ dlt_pipelines/
â”‚   â”‚   â””â”€â”€ sample_dlt_pipeline.py     # Sample DLT pipeline with quality checks
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ dlt_event_log_monitoring.py        # DLT event log collection
â”‚   â”‚   â”œâ”€â”€ system_tables_monitoring.py        # System tables integration
â”‚   â”‚   â”œâ”€â”€ lineage_tracking.py                # Lineage visualization
â”‚   â”‚   â””â”€â”€ alerting_configuration.py          # Alert setup and testing
â”‚   â”‚
â”‚   â””â”€â”€ data_quality/
â”‚       â””â”€â”€ quality_rules_manager.py    # Quality rules management
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_lakehouse_monitoring.py   # Lakehouse Monitoring automation
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ monitoring_dashboard_queries.sql  # DBSQL dashboard queries
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ quality_rules/                  # Quality rule configurations
â”‚
â””â”€â”€ dashboards/                         # DBSQL dashboard exports
```

## ğŸš€ Quick Start

### 1. Monitor a DLT Pipeline

```python
# Run DLT event log monitoring
%run ./notebooks/monitoring/dlt_event_log_monitoring

# Widget parameters:
# - catalog: main
# - schema: observability
# - pipeline_id: <your-pipeline-id>
# - days_back: 7
```

### 2. Set Up Quality Rules

```python
# Open quality rules manager
%run ./notebooks/data_quality/quality_rules_manager

# Create a rule
from quality_rules_manager import RuleTemplates

rule = RuleTemplates.not_null(
    dataset_name="customers",
    column_name="email",
    action="drop"
)

# Rules are automatically available to DLT pipelines
```

### 3. Track Data Lineage

```python
# Run lineage tracking notebook
%run ./notebooks/monitoring/lineage_tracking

# Widget parameters:
# - target_table: main.sample_pipeline.gold_customer_metrics
# - lineage_direction: both
# - max_depth: 3
```

### 4. Set Up Lakehouse Monitoring

```python
# Run monitoring setup script
%run ./scripts/setup_lakehouse_monitoring

# Widget parameters:
# - table_to_monitor: main.sample_pipeline.silver_customers
# - monitor_type: TimeSeries
# - timestamp_col: created_date
# - granularity: 1 day
```

### 5. Configure Alerts

```python
# Run alerting configuration
%run ./notebooks/monitoring/alerting_configuration

# Configure notification channels:
# - notification_email: data-team@company.com
# - slack_webhook: https://hooks.slack.com/services/...
# - teams_webhook: https://outlook.office.com/webhook/...
```

## ğŸ”§ Configuration

### Alert Thresholds

Edit thresholds in `notebooks/monitoring/alerting_configuration.py`:

```python
ALERT_THRESHOLDS = {
    "cost": {
        "daily_increase_percent": 20,
        "absolute_threshold": 1000,
    },
    "job_failure": {
        "failure_rate_percent": 10,
        "consecutive_failures": 3,
    },
    "data_quality": {
        "quality_score_threshold": 95,
        "failed_records_threshold": 1000,
    },
    "pipeline_freshness": {
        "hours_stale": 24,
        "critical_hours_stale": 48,
    }
}
```

### Notification Channels

Configure in notebook widgets or environment:

```python
# Email
notification_email = "data-team@company.com"

# Slack webhook
slack_webhook = "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# Microsoft Teams webhook
teams_webhook = "https://outlook.office.com/webhook/YOUR/WEBHOOK/URL"
```

## ğŸ“Š Usage Examples

### Example 1: Create Custom Quality Rules

```python
from quality_rules_manager import create_rule

# Create a custom validation rule
custom_rule = create_rule(
    dataset_name="orders",
    expectation_name="valid_order_total",
    expectation_type="custom",
    constraint_expression="order_total = (quantity * unit_price) + tax",
    action="warn",
    description="Ensure order total matches calculated value",
    severity="medium",
    tags={"category": "calculation", "owner": "finance-team"}
)

# Rule will be automatically applied in DLT pipelines
```

### Example 2: Query Cost Metrics

```python
# Analyze daily costs
daily_costs = spark.sql("""
    SELECT
        usage_date,
        sku_name,
        SUM(cost) as total_cost,
        SUM(usage_quantity) as total_dbu
    FROM main.observability.cost_metrics
    WHERE usage_date >= current_date() - INTERVAL 30 DAYS
    GROUP BY usage_date, sku_name
    ORDER BY usage_date DESC, total_cost DESC
""")

display(daily_costs)
```

### Example 3: Impact Analysis for Table Changes

```python
from lineage_tracking import analyze_impact

# Analyze impact of changing a table
impact = analyze_impact("main.sample_pipeline.silver_customers")

print(f"Total Downstream Tables: {impact['total_downstream']}")
print(f"Affected Catalogs: {', '.join(impact['affected_catalogs'])}")
print(f"Critical Endpoints: {', '.join(impact['critical_endpoints'])}")
```

### Example 4: Monitor Pipeline Health

```python
# Query pipeline health summary
health = spark.sql("""
    SELECT
        pipeline_name,
        COUNT(*) as total_runs,
        AVG(duration_seconds) / 60 as avg_duration_min,
        SUM(CASE WHEN status = 'COMPLETED' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as success_rate,
        SUM(error_count) as total_errors
    FROM main.observability.dlt_pipeline_health
    WHERE start_time >= current_timestamp() - INTERVAL 7 DAYS
    GROUP BY pipeline_name
""")

display(health)
```

## ğŸ“ˆ Monitoring Dashboards

### Create DBSQL Dashboards

1. Navigate to **SQL** â†’ **Dashboards** â†’ **Create Dashboard**
2. Import queries from `sql/monitoring_dashboard_queries.sql`
3. Create visualizations:
   - **Line Chart**: Daily cost trends
   - **Pie Chart**: Cost by service/SKU
   - **Bar Chart**: Top expensive jobs
   - **Gauge**: Data quality score
   - **Table**: Recent pipeline failures

### Recommended Dashboard Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pipeline Observability Dashboard               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Quality   â”‚  â”‚ Success   â”‚  â”‚  Cost     â”‚   â”‚
â”‚  â”‚  Score    â”‚  â”‚   Rate    â”‚  â”‚  Today    â”‚   â”‚
â”‚  â”‚   98%     â”‚  â”‚   95%     â”‚  â”‚  $1,234   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Daily Cost Trend (Line Chart)                  â”‚
â”‚  Cost by Service (Pie Chart)                    â”‚
â”‚  Pipeline Failures (Table)                      â”‚
â”‚  Quality Metrics Over Time (Line Chart)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Best Practices

### Data Quality
1. **Start Simple**: Begin with critical validations (not null, referential integrity)
2. **Iterative Improvement**: Add rules as you understand your data better
3. **Use Quarantine Tables**: Review invalid data to refine rules
4. **Monitor Rule Effectiveness**: Track pass rates and adjust thresholds

### Pipeline Design
1. **Layer Separation**: Keep bronze/silver/gold boundaries clear
2. **Idempotency**: Ensure pipelines can be re-run safely
3. **Incremental Processing**: Use DLT's incremental features for large datasets
4. **Error Handling**: Use appropriate expectation actions (warn vs. drop vs. fail)

### Monitoring
1. **Set Baselines**: Establish normal ranges for metrics before alerting
2. **Alert Fatigue**: Start with fewer, high-value alerts and expand
3. **Dashboard Design**: Focus on actionable metrics for each audience
4. **Regular Reviews**: Schedule weekly reviews of monitoring data

### Cost Optimization
1. **Rightsize Clusters**: Use monitoring data to optimize worker counts
2. **Schedule Pipelines**: Run during off-peak hours when possible
3. **Enable Photon**: For significant performance improvements
4. **Monitor Waste**: Identify idle clusters and redundant processing

## ğŸ” Troubleshooting

### Issue: System Tables Not Available

**Symptom**: `Table or view not found: system.billing.usage`

**Solution**:
- System tables must be enabled by Databricks account admin
- Check with your admin or contact Databricks support
- Alternative: Use Databricks usage APIs

### Issue: DLT Event Logs Not Found

**Symptom**: `event_log('pipelines/xxx') returns no data`

**Solution**:
- Verify pipeline ID is correct
- Ensure pipeline has run at least once
- Check permissions to read pipeline event logs
- Use `event_log('pipelines/*')` to see all pipelines

### Issue: Quality Rules Not Applied

**Symptom**: Expectations don't appear in DLT pipeline

**Solution**:
- Verify rules table path is correct
- Check rules are enabled (`is_enabled = TRUE`)
- Ensure dataset name matches exactly
- Restart DLT pipeline after adding rules

### Issue: Lakehouse Monitoring Fails

**Symptom**: `Monitor creation failed`

**Solution**:
- Verify table exists and has data
- Check timestamp column exists and is valid date/timestamp type
- Ensure you have permissions to create monitors
- Verify output schema exists and is accessible

### Issue: Alerts Not Firing

**Symptom**: No notifications received despite conditions met

**Solution**:
- Test notification channels separately
- Verify webhook URLs are correct
- Check DBSQL alert query returns expected results
- Confirm alert schedule is active
- Review alert history for errors

## ğŸ“š Additional Resources

### Databricks Documentation
- [Delta Live Tables](https://docs.databricks.com/delta-live-tables/index.html)
- [Unity Catalog Lineage](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html)
- [Lakehouse Monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html)
- [System Tables](https://docs.databricks.com/administration-guide/system-tables/index.html)

### Related Projects
- [Databricks Asset Bundles](https://docs.databricks.com/dev-tools/bundles/index.html) - For CI/CD deployment
- [dbt-databricks](https://github.com/databricks/dbt-databricks) - For dbt integration
- [Great Expectations](https://greatexpectations.io/) - Additional data quality framework

## ğŸ¤ Contributing

Contributions are welcome! To contribute:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly in a Databricks workspace
5. Submit a pull request with detailed description

## ğŸ“„ License

This project is provided as-is for educational and reference purposes.

## ğŸ™ Acknowledgments

Built following best practices from:
- Databricks Product Documentation
- Databricks Solution Accelerators
- Data Engineering community patterns

---

**Questions or Issues?**
- Review the [PRD](databricks_observability_and_data_quality_prd.md) for detailed requirements
- Check [Troubleshooting](#troubleshooting) section
- Open an issue in your repository

**Happy Monitoring! ğŸ‰**
